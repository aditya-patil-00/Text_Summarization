{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re   \n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>summary</th>\n",
       "      <th>dialogue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13818513</td>\n",
       "      <td>Amanda baked cookies and will bring Jerry some...</td>\n",
       "      <td>Amanda: I baked  cookies. Do you want some?\\r\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13728867</td>\n",
       "      <td>Olivia and Olivier are voting for liberals in ...</td>\n",
       "      <td>Olivia: Who are you voting for in this electio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13681000</td>\n",
       "      <td>Kim may try the pomodoro technique recommended...</td>\n",
       "      <td>Tim: Hi, what's up?\\r\\nKim: Bad mood tbh, I wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13730747</td>\n",
       "      <td>Edward thinks he is in love with Bella. Rachel...</td>\n",
       "      <td>Edward: Rachel, I think I'm in ove with Bella....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13728094</td>\n",
       "      <td>Sam is confused, because he overheard Rick com...</td>\n",
       "      <td>Sam: hey  overheard rick say something\\r\\nSam:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                            summary  \\\n",
       "0  13818513  Amanda baked cookies and will bring Jerry some...   \n",
       "1  13728867  Olivia and Olivier are voting for liberals in ...   \n",
       "2  13681000  Kim may try the pomodoro technique recommended...   \n",
       "3  13730747  Edward thinks he is in love with Bella. Rachel...   \n",
       "4  13728094  Sam is confused, because he overheard Rick com...   \n",
       "\n",
       "                                            dialogue  \n",
       "0  Amanda: I baked  cookies. Do you want some?\\r\\...  \n",
       "1  Olivia: Who are you voting for in this electio...  \n",
       "2  Tim: Hi, what's up?\\r\\nKim: Bad mood tbh, I wa...  \n",
       "3  Edward: Rachel, I think I'm in ove with Bella....  \n",
       "4  Sam: hey  overheard rick say something\\r\\nSam:...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_json('corpus/train.json')\n",
    "test_df = pd.read_json('corpus/test.json')\n",
    "val_df = pd.read_json('corpus/val.json')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14732, 3), (819, 3), (818, 3))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = train_df['dialogue'][:1000]\n",
    "summaries = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline Model\n",
    "\n",
    "We will use extractive summarization as our baseline model. We will use cosine similarity to find the similarity between the sentences and the document. We will then rank the sentences based on the similarity score and select the top N sentences as the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def baseline_summary_extractive(text, num_sentences=2):\n",
    "    sentences = sent_tokenize(text)\n",
    "    vectorizer = CountVectorizer().fit_transform(sentences)\n",
    "    vectors = vectorizer.toarray()\n",
    "    similarity_matrix = cosine_similarity(vectors)\n",
    "    sentence_scores = similarity_matrix.sum(axis=1)\n",
    "    ranked_sentences = [sentences[i] for i in sentence_scores.argsort()[-num_sentences:]]\n",
    "    return \"\\n\".join(ranked_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Amanda: I baked  cookies.\\nAmanda: I'll bring ...\n",
       "1           Oliver: Great\\nOliver: Liberals as always.\n",
       "2    Kim: Oh you know, uni stuff and unfucking my r...\n",
       "3    Edward: Rachel, I think I'm in ove with Bella....\n",
       "4    Naomi: i used to love living with you before i...\n",
       "Name: dialogue, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries['baseline'] = sample_text[0:5].apply(baseline_summary_extractive)\n",
    "summaries['baseline']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstractive Summarization\n",
    "\n",
    "We will use the seq2seq model with attention for abstractive summarization. We will use the encoder-decoder architecture with attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HuggingFace Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BART\n",
    "\n",
    "BART is a denoising autoencoder for pretraining sequence-to-sequence models. It is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Transformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56279947fa4945039ae12e428fedaa40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "844cd581a22d40d788dd61e8e5c6d0fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "221e63a71d954b12a2b6eb1a6bf25c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8101a7ea277845e0aed21259ef3dd8dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da9e2c9a458647f98f6bdab79d9f18da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f4cbccb6b84799adce9e01348387ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 100, but your input_length is only 31. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=15)\n",
      "Your max_length is set to 100, but your input_length is only 38. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n",
      "Your max_length is set to 100, but your input_length is only 50. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=25)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Amanda: I baked  cookies. Do you want some? Jerry: Sure!',\n",
       " \"Olivia asks Oliver who he is voting for in the election. Oliver says he's voting for Liberals as always. Olivia asks him who he will be voting for.\",\n",
       " \"Kim: I was going to do lots of stuff but ended up procrastinating. Tim: For doing stuff I recommend Pomodoro technique where u use breaks for doing chores. Kim: Maybe tomorrow I'll move my ass and do everything.\",\n",
       " \"Edward: Rachel, I think I'm in ove with Bella. Rachel: Dont say anything else.\",\n",
       " \"Sam overheard his roommate saying he wasn't very happy living with him. Sam says he doesn't know what to do about it. Sam's roommate says he should talk to him.\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline('summarization', model='facebook/bart-large-cnn', max_length=100, min_length=5)\n",
    "pipe_out = pipe(sample_text[0:5].tolist())\n",
    "summaries['bart'] = [out['summary_text'] for out in pipe_out]\n",
    "summaries['bart']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PEGASUS\n",
    "\n",
    "PEGASUS is a state-of-the-art abstractive summarization model. It is trained on a large corpus of unlabelled text such as news articles and scientific papers, and is able to generate coherent summaries from documents it has never seen before. PEGASUS is trained using a form of self-supervision called pre-training via back-translation. This means that it is trained to reconstruct human-written summaries of documents, and it learns to do so by reading millions of example document-summary pairs. PEGASUS is trained to predict the summary of a document from a corrupted version of the summary. This is done by corrupting the summary with an arbitrary noising function, and learning a model to reconstruct the original summary. This is similar to the denoising auto-encoder objective used in BART, but with a few key differences. First, PEGASUS is trained on a much larger corpus of data, and second, PEGASUS is trained to predict the summary of a document, rather than the document itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.encoder.embed_positions.weight', 'model.decoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Your max_length is set to 100, but your input_length is only 25. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=12)\n",
      "Your max_length is set to 100, but your input_length is only 26. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=13)\n",
      "Your max_length is set to 100, but your input_length is only 48. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Jerry: Hello, Amanda.',\n",
       " \"Olivia: Hi, I'm Olivia from Newsround and I'm here to answer your questions.\",\n",
       " \"Kim: Hi Tim, what's up?\",\n",
       " \"Rachel: I'm outside.\",\n",
       " \"In this week's episode of The Only Way Is Essex, Sam and Naomi are having a bit of a problem with each other.\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline('summarization', model='google/pegasus-xsum', max_length=100, min_length=5)\n",
    "pipe_out = pipe(sample_text[0:5].tolist())\n",
    "summaries['pegasus'] = [out['summary_text'] for out in pipe_out]\n",
    "summaries['pegasus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T5\n",
    "\n",
    "T5 is a text-to-text transformer model that uses the same architecture as BERT (bidirectional encoder) and GPT (left-to-right decoder). It is trained on a large corpus of unlabelled text such as news articles and scientific papers, and is able to generate coherent summaries from documents it has never seen before. T5 is trained using a form of self-supervision called pre-training via back-translation. This means that it is trained to reconstruct human-written summaries of documents, and it learns to do so by reading millions of example document-summary pairs. T5 is trained to predict the summary of a document from a corrupted version of the summary. This is done by corrupting the summary with an arbitrary noising function, and learning a model to reconstruct the original summary. This is similar to the denoising auto-encoder objective used in BART, but with a few key differences. First, T5 is trained on a much larger corpus of data, and second, T5 is trained to predict the summary of a document, rather than the document itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 100, but your input_length is only 28. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\n",
      "c:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:859: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Your max_length is set to 100, but your input_length is only 27. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=13)\n",
      "Your max_length is set to 100, but your input_length is only 55. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=27)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Jerry bakes cookies for her. She bakes them the next day.',\n",
       " \"Olivia asks Oliver who he's voting for in the election. Oliver says Liberals, as always.\",\n",
       " 'The next morning, Kim is in a bad mood and decides to do some housework instead of shopping. Tim suggests that she use a break from doing her chores to help her.',\n",
       " \"Edward tells Rachel that he's in love with Bella and wants to marry her immediately. She doesn't want to talk about it to anyone else so she leaves the room.\",\n",
       " \"Sam overhears a conversation between the two of them. It turns out that Mr. Rigby is upset about being left behind in London and is not happy with his new job. He's also unhappy at the fact that he has to live with another man who doesn't approve of him.\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline('summarization', model='pszemraj/long-t5-tglobal-base-16384-book-summary', max_length=100, min_length=5)\n",
    "pipe_out = pipe(sample_text[0:5].tolist())\n",
    "summaries['long-t5'] = [out['summary_text'] for out in pipe_out]\n",
    "summaries['long-t5']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROUGE and BLEU\n",
    "\n",
    "Bleu measures precision: how much the words (and/or n-grams) in the machine generated summaries appeared in the human reference summaries.\n",
    "\n",
    "Rouge measures recall: how much the words (and/or n-grams) in the human reference summaries appeared in the machine generated summaries.\n",
    "\n",
    "Precision can be seen as a measure of quality, and recall as a measure of quantity. Higher precision means that an algorithm returns more relevant results than irrelevant ones, and high recall means that an algorithm returns most of the relevant results (whether or not irrelevant ones are also returned)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROGUE\n",
    "\n",
    "The ROUGE score was specifically developed for applications like summarization where high recall is more important than just precision. \n",
    "\n",
    "ROUGE-N\n",
    "\n",
    "With ROUGE-N, the N represents the n-gram that we are using. For ROUGE-1 we would be measuring the match-rate of unigrams between our model output and reference. ROUGE-2 and ROUGE-3 would use bigrams and trigrams respectively.\n",
    "\n",
    "ROUGE-L\n",
    "\n",
    "ROUGE-L measures the longest common subsequence (LCS) between our model output and reference. All this means is that we count the longest sequence of tokens that is shared between both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya\\AppData\\Local\\Temp\\ipykernel_25160\\3509230384.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge_metric = load_metric(\"rouge\")\n",
      "Using the latest cached version of the module from C:\\Users\\Aditya\\.cache\\huggingface\\modules\\datasets_modules\\metrics\\rouge\\08e5f021b5761265deaafbf424e57913106427f546189fe3f934069dd32c153f (last modified on Sat Nov  4 16:29:38 2023) since it couldn't be found locally at rouge, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "rouge_metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge_dict  {'rouge1': 0.23972602739726026, 'rouge2': 0.027586206896551724, 'rougeL': 0.14383561643835616}\n",
      "rouge_dict  {'rouge1': 0.45098039215686275, 'rouge2': 0.10891089108910891, 'rougeL': 0.24509803921568626}\n",
      "rouge_dict  {'rouge1': 0.3076923076923077, 'rouge2': 0.015625, 'rougeL': 0.1384615384615385}\n",
      "rouge_dict  {'rouge1': 0.3529411764705882, 'rouge2': 0.06392694063926942, 'rougeL': 0.20814479638009048}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.239726</td>\n",
       "      <td>0.027586</td>\n",
       "      <td>0.143836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bart</th>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.108911</td>\n",
       "      <td>0.245098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pegasus</th>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.138462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long-t5</th>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.063927</td>\n",
       "      <td>0.208145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            rouge1    rouge2    rougeL\n",
       "baseline  0.239726  0.027586  0.143836\n",
       "bart      0.450980  0.108911  0.245098\n",
       "pegasus   0.307692  0.015625  0.138462\n",
       "long-t5   0.352941  0.063927  0.208145"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\"]\n",
    "\n",
    "reference = train_df['summary'][:1000]\n",
    "sample_ref = reference[0:5]\n",
    "\n",
    "records = []\n",
    "\n",
    "for model_name in summaries:\n",
    "    rouge_metric.add(prediction = summaries[model_name], reference = sample_ref )\n",
    "    score = rouge_metric.compute()\n",
    "    rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n",
    "    print('rouge_dict ', rouge_dict )\n",
    "    records.append(rouge_dict)\n",
    "\n",
    "pd.DataFrame.from_records(records, index = summaries.keys() )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:evaluate.loading:Using the latest cached version of the module from C:\\Users\\Aditya\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--bleu\\9e0985c1200e367cce45605ce0ecb5ede079894e0f24f54613fca08eeb8aff76 (last modified on Sat Nov  4 17:18:24 2023) since it couldn't be found locally at evaluate-metric--bleu, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "bleu_metric = evaluate.load('bleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bleu</th>\n",
       "      <th>precisions</th>\n",
       "      <th>brevity_penalty</th>\n",
       "      <th>length_ratio</th>\n",
       "      <th>translation_length</th>\n",
       "      <th>reference_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0.12916666666666668, 0.01702127659574468, 0.0...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.727273</td>\n",
       "      <td>240</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bart</th>\n",
       "      <td>0.048790</td>\n",
       "      <td>[0.29577464788732394, 0.08029197080291971, 0.0...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.613636</td>\n",
       "      <td>142</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pegasus</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0.26666666666666666, 0.01818181818181818, 0.0...</td>\n",
       "      <td>0.627089</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>60</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long-t5</th>\n",
       "      <td>0.031352</td>\n",
       "      <td>[0.23026315789473684, 0.04081632653061224, 0.0...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.727273</td>\n",
       "      <td>152</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              bleu                                         precisions  \\\n",
       "baseline  0.000000  [0.12916666666666668, 0.01702127659574468, 0.0...   \n",
       "bart      0.048790  [0.29577464788732394, 0.08029197080291971, 0.0...   \n",
       "pegasus   0.000000  [0.26666666666666666, 0.01818181818181818, 0.0...   \n",
       "long-t5   0.031352  [0.23026315789473684, 0.04081632653061224, 0.0...   \n",
       "\n",
       "          brevity_penalty  length_ratio  translation_length  reference_length  \n",
       "baseline         1.000000      2.727273                 240                88  \n",
       "bart             1.000000      1.613636                 142                88  \n",
       "pegasus          0.627089      0.681818                  60                88  \n",
       "long-t5          1.000000      1.727273                 152                88  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = []\n",
    "\n",
    "for model_name in summaries.keys():\n",
    "    results = bleu_metric.compute(predictions = summaries[model_name], references = sample_ref )\n",
    "    records.append(results)\n",
    "\n",
    "pd.DataFrame.from_records(records, index = summaries.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, bart is giving the best results. We will use bart for our final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
